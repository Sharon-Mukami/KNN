{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sharon-Mukami/KNN/blob/main/KNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Finding the Best K-value**"
      ],
      "metadata": {
        "id": "s8_Tv1kAd064"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Finding the best value for K in a K-Nearest Neighbors (KNN) algorithm is a crucial step in optimizing the model’s performance\n",
        "- The value of K determines how many neighbors to consider when classifying a new data point or making a regression prediction\n",
        "\n",
        "- Steps to Find the Best K:\n",
        "1. Split Data into Training and Validation Sets:\n",
        "\n",
        "2. Choose a Range of K Values:\n",
        "\n",
        "3. Train the Model with Different K Values:\n",
        "\n",
        "4. Use Cross-Validation:\n",
        "\n",
        "5. Evaluate the Results:\n",
        "\n",
        "6. Select the Best K:\n",
        "\n",
        "- Based on the evaluation metrics, choose the K that strikes the best balance between bias and variance\n",
        "- This is the point where the model generalizes well to new data."
      ],
      "metadata": {
        "id": "eky0aHzHhvn7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KNN with Scikit Learn"
      ],
      "metadata": {
        "id": "NHmGe5lMjSnJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- KNN is **easy to implement** and **works well for small datasets**\n",
        "- Choosing the right value of K is crucial, and **cross-validation helps select the optimal value**\n",
        "- Distance metrics and weighting can improve performance for certain problems"
      ],
      "metadata": {
        "id": "e8Swr448lAlu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n"
      ],
      "metadata": {
        "id": "FviNOiFSjWYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the dataset\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features (inputs)\n",
        "y = iris.target  # Target labels (outputs)\n"
      ],
      "metadata": {
        "id": "w42KDlyBjXAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123)\n"
      ],
      "metadata": {
        "id": "b5m7_pnBjW85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choosing a value of K\n",
        "# Instantiate the KNN classifier with K=3\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n"
      ],
      "metadata": {
        "id": "YxY33tv6jW6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the KNN model to the training data\n",
        "knn.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "1lXbdfd4jq3m",
        "outputId": "f6b9c8eb-a795-4b57-e340-ba4942288c68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsClassifier(n_neighbors=3)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=3)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test data\n",
        "y_pred = knn.predict(X_test)\n"
      ],
      "metadata": {
        "id": "JgwCeJqLj4Zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making predictions and Calculating the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XafQl2cPjqz3",
        "outputId": "c5cb9545-a50b-4f8b-84f8-84fc563dcfe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing K values from 1 to 20\n",
        "for k in range(1, 21):\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_train, y_train)\n",
        "    y_pred = knn.predict(X_test)\n",
        "    print(f\"K={k}, Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylxqr1UOjqxj",
        "outputId": "6e1b7b62-4423-417a-c28d-6559ba1a0eb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K=1, Accuracy: 0.89\n",
            "K=2, Accuracy: 0.89\n",
            "K=3, Accuracy: 0.95\n",
            "K=4, Accuracy: 0.95\n",
            "K=5, Accuracy: 0.97\n",
            "K=6, Accuracy: 0.95\n",
            "K=7, Accuracy: 0.97\n",
            "K=8, Accuracy: 0.97\n",
            "K=9, Accuracy: 0.97\n",
            "K=10, Accuracy: 0.95\n",
            "K=11, Accuracy: 0.97\n",
            "K=12, Accuracy: 0.97\n",
            "K=13, Accuracy: 0.97\n",
            "K=14, Accuracy: 0.97\n",
            "K=15, Accuracy: 0.97\n",
            "K=16, Accuracy: 0.97\n",
            "K=17, Accuracy: 0.97\n",
            "K=18, Accuracy: 0.92\n",
            "K=19, Accuracy: 0.95\n",
            "K=20, Accuracy: 0.89\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using KNN for regression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "# Instantiate the KNN regressor\n",
        "knn_regressor = KNeighborsRegressor(n_neighbors=3)\n",
        "\n",
        "# Fit the regressor model\n",
        "knn_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_reg = knn_regressor.predict(X_test)\n",
        "\n",
        "# Evaluate the regressor (e.g., using mean squared error)\n",
        "from sklearn.metrics import mean_squared_error\n",
        "mse = mean_squared_error(y_test, y_pred_reg)\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efc3aThtkVq1",
        "outputId": "69b71b40-a203-413b-bfb4-edbb1fa6132d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Advanced Options: Distance Metrics, Weights**\n",
        "\n",
        "You can customize the distance metric by setting the metric parameter (**e.g., 'euclidean', 'manhattan'**).\n",
        "You can also use weighted KNN, where closer neighbors contribute more to the decision than further ones, by setting **weights='distance'**."
      ],
      "metadata": {
        "id": "o0R8KQaSklFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Weighted KNN with Euclidean distance\n",
        "knn_weighted = KNeighborsClassifier(n_neighbors=3, weights='distance', metric='euclidean')\n",
        "knn_weighted.fit(X_train, y_train)\n",
        "y_pred_weighted = knn_weighted.predict(X_test)\n",
        "\n",
        "# Check accuracy\n",
        "accuracy_weighted = accuracy_score(y_test, y_pred_weighted)\n",
        "print(f\"Weighted KNN Accuracy: {accuracy_weighted:.2f}\")\n"
      ],
      "metadata": {
        "id": "lngv2fwtkhSH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af79fa10-24cf-4c2d-bb86-da55aa4ca11a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weighted KNN Accuracy: 0.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross-Validation for K Selection\n",
        "Cross-validation to find the best K by evaluating the performance across multiple folds."
      ],
      "metadata": {
        "id": "mJX86xtfkuKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Test K values from 1 to 10 using 5-fold cross-validation\n",
        "k_range = range(1, 11)\n",
        "k_scores = []\n",
        "\n",
        "for k in k_range:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    scores = cross_val_score(knn, X, y, cv=5, scoring='accuracy')\n",
        "    k_scores.append(scores.mean())\n",
        "\n",
        "print(f\"Best K value: {k_range[np.argmax(k_scores)]}\")\n"
      ],
      "metadata": {
        "id": "CMKSrulck1p-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8ed7bc8-df71-4220-b284-babcda762edc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best K value: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Considerations:**\n",
        "1. **Odd K Values:** If you’re dealing with classification, it’s usually good to use odd values of K to avoid ties when there are an equal number of nearest neighbors from different classes\n",
        "2. **Size of Dataset:** For larger datasets, a larger K value may provide more stable predictions.\n",
        "3. **Distance Metric:** The choice of the distance metric (Euclidean, Manhattan, etc.) can also affect the performance, so it’s worth testing different metrics when optimizing K."
      ],
      "metadata": {
        "id": "H4BdJnz7iS3s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How is KNN related to the Curse of Dimensionality?\n",
        "\n",
        "K-Nearest Neighbors (KNN) is highly sensitive to the curse of dimensionality, a phenomenon that occurs when data exists in a high-dimensional space, making it harder to analyze and interpret. Here's how KNN and the curse of dimensionality are related:\n",
        "\n",
        "1. **Distance Metrics Become Less Informative**\n",
        "KNN relies on calculating distances between data points to classify or make predictions, typically using metrics like Euclidean distance. In high-dimensional spaces, the distance between points increases, and all points tend to become equally distant from each other. This dilutes the meaningfulness of proximity, which is the core concept of KNN.\n",
        "\n",
        "**Effect:** In higher dimensions, the differences in distance between the nearest neighbors and farthest points become smaller, making it difficult for the algorithm to distinguish between relevant and irrelevant points.\n",
        "\n",
        "2. **Increased Sparsity of Data**\n",
        "As the number of dimensions (features) increases, the volume of the space grows exponentially, but the amount of data typically remains constant. This makes the data points more sparsely distributed across the feature space.\n",
        "\n",
        "**Effect:** With sparser data in high dimensions, the neighbors of a point may be far away or irrelevant, reducing the ability of KNN to accurately classify or predict. The model becomes less reliable because it doesn't have enough nearby points to make meaningful decisions.\n",
        "\n",
        "3. **Overfitting Risk**\n",
        "In higher dimensions, each data point may become isolated, with only a few close neighbors. This can lead to overfitting because KNN may memorize specific points and produce poor generalization on new data.\n",
        "\n",
        "**Effect:** If the number of dimensions grows, the model may classify points based on very few or irrelevant neighbors, leading to high variance in the model and overfitting.\n",
        "\n",
        "4. **Increased Computational Complexity**\n",
        "As the dimensionality increases, the number of calculations required to compute distances grows exponentially. KNN performs poorly with respect to computational efficiency in high-dimensional spaces due to the need to calculate distances between all points for each prediction.\n",
        "\n",
        "**Effect:** This results in slower performance and greater resource consumption when the data has many features.\n",
        "\n",
        "\n",
        "### Strategies to Mitigate the Curse of Dimensionality in KNN\n",
        "\n",
        "**Dimensionality Reduction:**\n",
        "\n",
        "Use techniques like Principal Component Analysis (PCA) or t-SNE to reduce the number of features while preserving as much of the data's variance or structure as possible.\n",
        "Feature selection methods can help identify the most important features and discard irrelevant ones\n",
        "\n",
        "\n",
        "**Distance Metric Optimization:**\n",
        "\n",
        "Try different distance metrics (e.g., Manhattan distance) to see which one performs better in high-dimensional spaces.\n",
        "Some high-dimensional problems may benefit from using Mahalanobis distance, which accounts for correlations between features.\n",
        "\n",
        "\n",
        "**Normalize the Data:**\n",
        "\n",
        "In high-dimensional spaces, it’s essential to ensure that all features contribute equally by scaling them (e.g., using standardization or min-max normalization) before calculating distances.\n",
        "\n",
        "**Use Dimensionality-Sensitive Algorithms:**\n",
        "\n",
        "Consider algorithms better suited for high-dimensional data, like Support Vector Machines (SVM) or Random Forests, if KNN struggles.\n",
        "\n",
        "\n",
        "- Lastly, KNN’s reliance on distance calculations makes it particularly vulnerable to the curse of dimensionality\n",
        "- As the number of features increases, distances between points become less meaningful, data becomes sparse, and the computational cost increases\n",
        "- Using dimensionality reduction, feature selection, and normalization can help mitigate these challenges."
      ],
      "metadata": {
        "id": "XQK3nQ0scoPv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Challenge - Use an Actual Dataset following the steps above"
      ],
      "metadata": {
        "id": "4o0EdPxqdfuS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model tuning and pipelines**\n",
        "\n",
        "Model Tuning are essential components of machine learning workflows, helping to optimize model performance and streamline the process.\n",
        "\n",
        "Here's an overview of each and how they work together.\n",
        "\n",
        "1. **Model Tuning**\n",
        "Model tuning is the process of optimizing the hyperparameters of a machine learning algorithm to improve its performance.\n",
        "\n",
        "Hyperparameters are parameters that are not learned by the model directly during training but are set before training begins.\n",
        "\n",
        "**Common Model Tuning Techniques:**\n",
        "\n",
        "**Grid Search:**\n",
        "\n",
        "Exhaustively searches through a manually specified subset of hyperparameters.\n",
        "Trains and evaluates the model for each combination to find the best one.\n",
        "\n",
        "**Random Search:**\n",
        "\n",
        "Samples random combinations of hyperparameters from a given range.\n",
        "More efficient than grid search for high-dimensional hyperparameter spaces.\n",
        "\n",
        "**Bayesian Optimization:**\n",
        "\n",
        "Uses a probabilistic model to select the next set of hyperparameters based on past evaluations.\n",
        "More efficient than grid and random search as it intelligently navigates the search space.\n",
        "\n",
        "**Hyperopt, Optuna, and other advanced libraries:**\n",
        "\n",
        "Libraries like Hyperopt and Optuna offer more sophisticated optimization techniques, including Bayesian optimization, tree-structured parzen estimators (TPE), and adaptive sampling."
      ],
      "metadata": {
        "id": "6raP8q6RxuHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Example classifier\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# Define a grid of hyperparameters\n",
        "param_grid = {'n_neighbors': [3, 5, 7, 9], 'weights': ['uniform', 'distance']}\n",
        "\n",
        "# Grid search\n",
        "grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best hyperparameters\n",
        "print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
        "print(f\"Best Accuracy: {grid_search.best_score_}\")\n"
      ],
      "metadata": {
        "id": "FpEoivJmxw1G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54b3e897-7a9b-449f-b776-2e37f77cf6ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'n_neighbors': 3, 'weights': 'uniform'}\n",
            "Best Accuracy: 0.9818181818181818\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Pipelines**\n",
        "\n",
        "A pipeline is a sequence of steps where data is preprocessed and passed to a model for training and evaluation in a systematic way.\n",
        "\n",
        "Using pipelines ensures that all steps are executed in the right order and that there's no data leakage between training and testing.\n",
        "\n",
        "**Benefits of Using Pipelines:**\n",
        "\n",
        "- Reproducibility: Ensures consistent application of data transformations.\n",
        "- Simplifies Code: Consolidates multiple steps (data preprocessing, model training, etc.) into a single process.\n",
        "- Prevents Data Leakage: Ensures transformations like scaling are applied only to the training data and not to the test data during training\n",
        "\n",
        "Using Pipelines in Scikit-learn"
      ],
      "metadata": {
        "id": "R_BIwl6dx0C4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Define the steps in the pipeline\n",
        "pipe = Pipeline([\n",
        "    ('scaler', StandardScaler()),  # Step 1: Standardize the data\n",
        "    ('knn', KNeighborsClassifier(n_neighbors=5))  # Step 2: Train the KNN model\n",
        "])\n",
        "\n",
        "# Train the pipeline\n",
        "pipe.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "y_pred = pipe.predict(X_test)\n"
      ],
      "metadata": {
        "id": "zaN7mC-5xv9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pipelines with Model Tuning**\n",
        "\n",
        "You can combine pipelines with hyperparameter tuning using GridSearchCV or RandomizedSearchCV\n",
        "This allows you to tune hyperparameters of both the preprocessing steps (like scaling) and the model itself."
      ],
      "metadata": {
        "id": "xDD8TMruyAzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Define a pipeline with scaling and KNN\n",
        "pipe = Pipeline([\n",
        "    ('scaler', StandardScaler()),  # Standardize data\n",
        "    ('knn', KNeighborsClassifier())  # KNN model\n",
        "])\n",
        "\n",
        "# Define a parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'knn__n_neighbors': [3, 5, 7],  # Tuning the number of neighbors\n",
        "    'knn__weights': ['uniform', 'distance'],  # Tuning the weight function\n",
        "}\n",
        "\n",
        "# Perform grid search\n",
        "grid_search = GridSearchCV(pipe, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Output the best parameters and model performance\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best score: {grid_search.best_score_}\")\n"
      ],
      "metadata": {
        "id": "WYLBFGCvyEYc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dee76435-8631-4523-a20d-2238d7f70a73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'knn__n_neighbors': 5, 'knn__weights': 'uniform'}\n",
            "Best score: 0.9731225296442687\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Advanced Techniques:** Cross-Validation and Nested Cross-Validation\n",
        "Cross-validation (CV): Used to evaluate model performance by splitting the dataset into training and validation sets multiple times.\n",
        "It helps prevent **overfitting and ensures that the model generalizes well to unseen data.**\n",
        "\n",
        "**Nested cross-validation: **This combines model tuning and cross-validation in a way that avoids overfitting during hyperparameter tuning.\n",
        "It involves an **outer loop for model evaluation and an inner loop for hyperparameter tuning**."
      ],
      "metadata": {
        "id": "qgogBKsXyIyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using nested cross-val\n",
        "\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
        "\n",
        "# Define the pipeline\n",
        "pipe = Pipeline([\n",
        "    ('scaler', StandardScaler()),  # Standardize the features\n",
        "    ('knn', KNeighborsClassifier())  # KNN classifier\n",
        "])\n",
        "\n",
        "# Parameter grid for Grid Search\n",
        "param_grid = {'knn__n_neighbors': [3, 5, 7], 'knn__weights': ['uniform', 'distance']}\n",
        "\n",
        "# Perform GridSearchCV\n",
        "grid_search = GridSearchCV(pipe, param_grid, cv=5)\n",
        "\n",
        "# Nested cross-validation (with 10 outer folds)\n",
        "nested_cv_scores = cross_val_score(grid_search, X, y, cv=10)\n",
        "\n",
        "# Print the nested cross-validation score\n",
        "print(f\"Nested CV accuracy: {np.mean(nested_cv_scores):.2f}\")\n"
      ],
      "metadata": {
        "id": "Mb8imH_TyNWl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1357235-9df0-408d-b894-afcc2d1cb190"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nested CV accuracy: 0.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Summary\n",
        "\n",
        "- Model Tuning is the process of optimizing hyperparameters to improve the performance of a machine learning model.\n",
        "- Pipelines organize the workflow by combining preprocessing steps with model training in a structured manner\n",
        "- Combining these concepts (pipelines and model tuning) helps ensure that the entire machine learning process is systematic, reproducible, and efficient\n",
        "- Additionally, using cross-validation ensures robust model evaluation, preventing overfitting and improving generalization."
      ],
      "metadata": {
        "id": "amhGUUutyZMj"
      }
    }
  ]
}